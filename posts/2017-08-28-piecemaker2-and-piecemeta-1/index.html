<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Piecemaker2 and Piecemeta 1 | CCOnline</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=generator content="Hugo 0.74.3"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link href=/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css rel=stylesheet><meta property="og:title" content="Piecemaker2 and Piecemeta 1"><meta property="og:description" content="report"><meta property="og:type" content="article"><meta property="og:url" content="https://cconline.naotohieda.com/posts/2017-08-28-piecemaker2-and-piecemeta-1/"><meta property="article:published_time" content="2017-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2017-08-28T00:00:00+00:00"><meta itemprop=name content="Piecemaker2 and Piecemeta 1"><meta itemprop=description content="report"><meta itemprop=datePublished content="2017-08-28T00:00:00+00:00"><meta itemprop=dateModified content="2017-08-28T00:00:00+00:00"><meta itemprop=wordCount content="1165"><meta itemprop=keywords content="annotation,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Piecemaker2 and Piecemeta 1"><meta name=twitter:description content="report"></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">CCOnline</a><div class="flex-l items-center"></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class=mt3><a href="https://www.facebook.com/sharer.php?u=https://cconline.naotohieda.com/posts/2017-08-28-piecemaker2-and-piecemeta-1/" class="facebook no-underline" aria-label="share on Facebook"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765 50.32h6.744V33.998h4.499l.596-5.624h-5.095l.007-2.816c0-1.466.14-2.253 2.244-2.253h2.812V17.68h-4.5c-5.405.0-7.307 2.729-7.307 7.317v3.377h-3.369v5.625h3.369V50.32zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd"/></svg></a><a href="https://twitter.com/share?url=https://cconline.naotohieda.com/posts/2017-08-28-piecemaker2-and-piecemeta-1/&text=Piecemaker2%20and%20Piecemeta%201" class="twitter no-underline" aria-label="share on Twitter"><svg height="32" style="enable-background:new 0 0 67 67" viewBox="0 0 67 67" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167 22.283c-2.619.953-4.274 3.411-4.086 6.101l.063 1.038-1.048-.127c-3.813-.487-7.145-2.139-9.974-4.915l-1.383-1.377-.356 1.017c-.754 2.267-.272 4.661 1.299 6.271.838.89.649 1.017-.796.487-.503-.169-.943-.296-.985-.233-.146.149.356 2.076.754 2.839.545 1.06 1.655 2.097 2.871 2.712l1.027.487-1.215.021c-1.173.0-1.215.021-1.089.467.419 1.377 2.074 2.839 3.918 3.475l1.299.444-1.131.678c-1.676.976-3.646 1.526-5.616 1.568C19.775 43.256 19 43.341 19 43.405c0 .211 2.557 1.397 4.044 1.864 4.463 1.377 9.765.783 13.746-1.568 2.829-1.673 5.657-5 6.978-8.221.713-1.716 1.425-4.851 1.425-6.354.0-.975.063-1.102 1.236-2.267.692-.678 1.341-1.419 1.467-1.631.21-.403.188-.403-.88-.043-1.781.636-2.033.551-1.152-.402.649-.678 1.425-1.907 1.425-2.267.0-.063-.314.042-.671.233-.377.212-1.215.53-1.844.72l-1.131.361-1.027-.7c-.566-.381-1.361-.805-1.781-.932C39.766 21.902 38.131 21.944 37.167 22.283zM33 64C16.432 64 3 50.569 3 34S16.432 4 33 4s30 13.431 30 30S49.568 64 33 64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://cconline.naotohieda.com/posts/2017-08-28-piecemaker2-and-piecemeta-1/&title=Piecemaker2%20and%20Piecemeta%201" class="linkedin no-underline" aria-label="share on LinkedIn"><svg height="32" style="enable-background:new 0 0 65 65" viewBox="0 0 65 65" width="32" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M50.837 48.137V36.425c0-6.275-3.35-9.195-7.816-9.195-3.604.0-5.219 1.983-6.119 3.374V27.71h-6.79c.09 1.917.0 20.427.0 20.427h6.79V36.729c0-.609.044-1.219.224-1.655.49-1.22 1.607-2.483 3.482-2.483 2.458.0 3.44 1.873 3.44 4.618v10.929H50.837zM22.959 24.922c2.367.0 3.842-1.57 3.842-3.531-.044-2.003-1.475-3.528-3.797-3.528s-3.841 1.524-3.841 3.528c0 1.961 1.474 3.531 3.753 3.531H22.959zM34 64C17.432 64 4 50.568 4 34 4 17.431 17.432 4 34 4s30 13.431 30 30C64 50.568 50.568 64 34 64zM26.354 48.137V27.71h-6.789v20.427H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:"/></svg></a></div><h1 class="f1 athelas mt3 mb1">Piecemaker2 and Piecemeta 1</h1><p class=tracked>By <strong>Naoto Hieda</strong></p><time class="f6 mv4 dib tracked" datetime=2017-08-28T00:00:00Z>August 28, 2017</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p><img src=/images/2017-08-28-piecemaker2-and-piecemeta-1.jpg alt></p><p>The project &ldquo;Generative Pathways&rdquo; is a collaboration between Naoto Hieda and Lisa Parra starting in September 2017. Extending the concept of <a href=https://naotohieda.com/blog/eeg-experiments>EEG Experiments</a>, the initial aim is to research the intersection of geometric choreography and neurotechnology. There are two elements. First, we need a computer program to inspire or to choreograph a dancer. Fortunately we have access to the source code of <a href=http://waltzbinaire.com/work/pathfinder/>Pathfinder</a> project, which later I rewrote as <a href=https://github.com/micuat/Pathrefinder>Pathrefinder</a>. Second, the movements will be archived in the forms of video, movement data and bio/neurosignals to be reviewed and analyzed later on. We had discussions with <a href=http://motionbank.org/>Motion Bank</a> through Choreographic Coding Labs and a meeting at its headquarters and we decided to use <a href=http://motionbank.org/en/content/education-piecemaker>Piecemaker2</a> and <a href=https://app.piecemeta.com/>Piecemeta</a> platforms. Piecemaker2 is a web service to archive videos and annotations in a single timeline. Piecemeta is also a web platform but to store numeric data with constant frame rate (e.g., motion tracking). To create an account and to receive a tutorial guide, please contact Motion Bank directly. In this article, I would like to describe my initial attempt to use Piecemaker2/Piecemeta and the current issues.</p><p>To begin with, I danced with Pathfinder projected on a wall and recorded it with Yi Action Camera and Kinect 2. Therefore, the available data are:</p><ul><li>Video (action camera)</li><li>Depth video (Kinect)</li><li>Skeletal tracking (Kinect)</li><li>Pathfinder score</li></ul><p>The following video shows how the skeleton (left) and video from the action camera (right) appear:</p><blockquote class=instagram-media data-instgrm-version=7 style="background:#fff;border:0;border-radius:3px;box-shadow:0 0 1px 0 rgba(0,0,0,.5),0 1px 10px 0 rgba(0,0,0,.15);margin:1px;max-width:658px;padding:0;width:99.375%;width:-webkit-calc(100% - 2px);width:calc(100% - 2px)"><div style=padding:8px><div style="background:#f8f8f8;line-height:0;margin-top:40px;padding:28.194444444444443% 0;text-align:center;width:100%"><div style="background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC);display:block;height:44px;margin:0 auto -44px;position:relative;top:-22px;width:44px"></div></div><p style="color:#c9c8cd;font-family:Arial,sans-serif;font-size:14px;line-height:17px;margin-bottom:0;margin-top:8px;overflow:hidden;padding:8px 0 7px;text-align:center;text-overflow:ellipsis;white-space:nowrap"><a href=https://www.instagram.com/p/BYMxLWsFmLs/ style=color:#c9c8cd;font-family:Arial,sans-serif;font-size:14px;font-style:normal;font-weight:400;line-height:17px;text-decoration:none target=_blank>A post shared by Naoto Hi√©da (@micuat)</a> on <time style=font-family:Arial,sans-serif;font-size:14px;line-height:17px datetime=2017-08-25T02:10:19+00:00>Aug 24, 2017 at 7:10pm PDT</time></p></div></blockquote><script async defer src=//platform.instagram.com/en_US/embeds.js></script><p>The data seem to be synchronized in the video but it is because I manually started playing back the Kinect recording and action camera video at the right time. Also they are two separate programs (skeletal tracking and video player), which should be shown in a single program. The goal is to use Piecemaker2/Piecemeta so that one can review all the data streams in sync by simply hitting a play button (or even navigate through a timeline). Challenges are:</p><ul><li>Uploading videos and aligning the timestamps</li><li>Outputting the Pathfinder score and uploading it to Piecemaker2/Piecemeta</li><li>Extracting skeletal tracking and uploading it to Piecemeta</li></ul><h2 id=uploading-videos-and-aligning-the-timestamps>Uploading videos and aligning the timestamps</h2><p>The easiest way is to upload the videos on YouTube (if you wish you can select &ldquo;unlisted&rdquo; so only those who have a URL can watch the video) and embed them to Piecemaker2. The problem is aligning two or more videos. One solution is to use <a href=https://github.com/motionbank/piecemaker-processing/tree/master/time-sync/qr_sync_encoder_html>QR codes</a> to visually encode timestamps, which should look like this:</p><div class=youtube-container><iframe width=560 height=315 src=https://www.youtube.com/embed/j9Ln_iIhK5Q frameborder=0 allowfullscreen></iframe></div><p>This is a good solution for accurate timing but you need to find a video frame a with visible QR code, decode the timestamp and calculate the timestamp at the first frame. So far I have not found an automation script to do this, and hopefully I have time to write it soon (<strong>TODO1</strong>). Alternatively, if your camera has an audio input jack, which my action camera does not have, you can use linear timecode to make your life easier. This time I decided to manually align timestamps by marking the frame in video when I clapped my hands.</p><h2 id=outputting-the-pathfinder-score-and-uploading-it-to-piecemaker2piecemeta>Outputting the Pathfinder score and uploading it to Piecemaker2/Piecemeta</h2><p>I modified the Pathfinder code so that it dumps the score as text:</p><pre><code>26.124 r: +00.000 -&gt; +01.571 tx: -06.000 -&gt; +00.000 ... tri: +00.000 -&gt; +01.000
</code></pre><p>The first field is the timestamp in seconds since Pathfinder is launched. Then labels &ldquo;r&rdquo;, &ldquo;tx&rdquo;, &ldquo;ty&rdquo;, &ldquo;sx&rdquo;, &ldquo;sy&rdquo; and &ldquo;tri&rdquo; follow, which are rotation, translation (x/y), scaling (x/y) and triangle deformation parameters, respectively. Every parameter is followed by &ldquo;origin&rdquo; -> &ldquo;destination&rdquo; so that from this score, one can recover the Pathfinder geometry, which is not implemented yet (<strong>TODO2</strong>). To align the timestamps with the video, I extracted the timestamp of the video from Piecemaker2 (something looks like 1503625428.717). Again I manually marked when the Pathfinder starts in the video. So, for example, if the video&rsquo;s timestamp is 1503625428.717 and Pathfinder starts after 3.5 seconds, the score should start at 1503625432.217. I assume that this can be simplified and automated if the clocks of the camera and a computer that runs Pathfinder are synchronized. Then,with no effort, absolute timestamps should be aligned (<strong>TODO3</strong>).</p><p>I found uploading the score data to Piecemaker2/Piecemeta to be challenging. I manually made &ldquo;scene&rdquo; markers on Piecemaker2 with the corrected timestamps. The advantage of Piecemaker2 is that you can jump from one scene of the video to another just by clicking on the scenes on the web platform. However, it seems there is no API to upload markers so if you have tens or hundreds of scenes, this is not practical. Next time I will use Piecemeta instead. The advantage is that data upload can be simplified by outputting csv or trac data (explained later). Nevertheless, you need to develop your own app to review the data with the video since the interface on Piecemeta is quite basic. Also you need to reformat the score into data with a constant framerate, say 30 frames per second, while points between the score can vary (usually 6-12 seconds).</p><h2 id=extracting-skeletal-tracking-and-uploading-it-to-piecemeta>Extracting skeletal tracking and uploading it to Piecemeta</h2><p>Kinect SDK comes with Kinect Studio, which allows you to record color and depth streams in an xef file and play them back from apps that use Kinect SDK. It is a powerful tool, and I used it for recording. As a post process, the recorded data is streamed to <a href=https://github.com/Zillode/OSCeleton-KinectSDK2>OSCeleton</a> and output to a csv file using it&rsquo;s logging feature (you need to enable logging from a command line option or use <a href=https://github.com/micuat/OSCeleton-KinectSDK2/tree/piecemeta>my branch</a>). This branch also includes a <a href=https://github.com/micuat/OSCeleton-KinectSDK2/blob/piecemeta/osceleton_piecemeta/osceleton_piecemeta.py>script</a> to convert the output csv file into a trac format that is supported by Piecemeta. Once converted into trac, the joints are automatically labeled (e.g., SpineShoulder / X). You can find some data <a href=https://app.piecemeta.com/packages/d06fa439-1218-49b3-b3eb-69a5011840f4/show>here</a>.</p><p>The only issue is timestamping. Since the time when you hit the play button on Kinect Studio to stream data to OSCeleton is arbitrary, it is almost impossible to programmatically align the timestamp of the skeletal tracking and the original video. It seems there is a <a href=https://msdn.microsoft.com/en-us/library/dn785306.aspx>Kinect Studio API</a> so you may be able to extract the timestamps when they are recorded. But this also means that modification to the OSCeleton code is needed (<strong>TODO4</strong>). Another problem with timestamping is that OSCeleton is not running at the fixed framerate so you need to upsample the output skeletal tracking data to meet your desired framerate (<strong>TODO5</strong>).</p><p>Currently I have problems described above but, at least, I succeeded in retrieving the uploaded data from Piecemaker2/Piecemeta and playing back the video with skeletal tracking by modifying <a href=https://gist.github.com/fjenett/248d1d7ccfb8414c54a5>the Processing sketch</a> by Florian Jenett:</p><blockquote class=instagram-media data-instgrm-version=7 style="background:#fff;border:0;border-radius:3px;box-shadow:0 0 1px 0 rgba(0,0,0,.5),0 1px 10px 0 rgba(0,0,0,.15);margin:1px;max-width:658px;padding:0;width:99.375%;width:-webkit-calc(100% - 2px);width:calc(100% - 2px)"><div style=padding:8px><div style="background:#f8f8f8;line-height:0;margin-top:40px;padding:26.180555555555557% 0;text-align:center;width:100%"><div style="background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAsCAMAAAApWqozAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAMUExURczMzPf399fX1+bm5mzY9AMAAADiSURBVDjLvZXbEsMgCES5/P8/t9FuRVCRmU73JWlzosgSIIZURCjo/ad+EQJJB4Hv8BFt+IDpQoCx1wjOSBFhh2XssxEIYn3ulI/6MNReE07UIWJEv8UEOWDS88LY97kqyTliJKKtuYBbruAyVh5wOHiXmpi5we58Ek028czwyuQdLKPG1Bkb4NnM+VeAnfHqn1k4+GPT6uGQcvu2h2OVuIf/gWUFyy8OWEpdyZSa3aVCqpVoVvzZZ2VTnn2wU8qzVjDDetO90GSy9mVLqtgYSy231MxrY6I2gGqjrTY0L8fxCxfCBbhWrsYYAAAAAElFTkSuQmCC);display:block;height:44px;margin:0 auto -44px;position:relative;top:-22px;width:44px"></div></div><p style="color:#c9c8cd;font-family:Arial,sans-serif;font-size:14px;line-height:17px;margin-bottom:0;margin-top:8px;overflow:hidden;padding:8px 0 7px;text-align:center;text-overflow:ellipsis;white-space:nowrap"><a href=https://www.instagram.com/p/BYTUqIiFAbk/ style=color:#c9c8cd;font-family:Arial,sans-serif;font-size:14px;font-style:normal;font-weight:400;line-height:17px;text-decoration:none target=_blank>A post shared by Naoto Hi√©da (@micuat)</a> on <time style=font-family:Arial,sans-serif;font-size:14px;line-height:17px datetime=2017-08-27T15:15:47+00:00>Aug 27, 2017 at 8:15am PDT</time></p></div></blockquote><script async defer src=//platform.instagram.com/en_US/embeds.js></script><p>Obviously data are not in sync, but I wanted to write this report in order to clarify where I am and what has to be done. Hopefully in the next post I will have a better picture of the workflow to record, to upload and to review multiple data streams.</p><ul class=pa0><li class=list><a href=/tags/annotation class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">annotation</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://cconline.naotohieda.com/>&copy; CCOnline 2020</a><div></div></div></footer><script src=/dist/js/app.3fc0f988d21662902933.js></script></body></html>